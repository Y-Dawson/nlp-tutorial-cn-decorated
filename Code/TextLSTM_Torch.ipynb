{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "input_batch被处理为的形状是: torch.Size([10, 3, 26])\n"
    }
   ],
   "source": [
    "'''\n",
    "  code by Tae Hwan Jung(Jeff Jung) @graykode\n",
    "  参考了 理解Pytorch中LSTM的输入输出参数含义  AutoML机器学习\n",
    "'''\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "char_arr = [c for c in 'abcdefghijklmnopqrstuvwxyz'] #输出一个每个字母的list\n",
    "word_dict = {n: i for i, n in enumerate(char_arr)} #word->index\n",
    "number_dict = {i: w for i, w in enumerate(char_arr)}#index->word\n",
    "n_class = len(word_dict) # number of class(=number of vocab) #分类长度26\n",
    "\n",
    "seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']\n",
    "\n",
    "# TextLSTM Parameters\n",
    "n_step = 3  \n",
    "n_hidden = 8 #单词向量维度\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch, target_batch = [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        input = [word_dict[n] for n in seq[:-1]] # 'm', 'a' , 'k' is input\n",
    "        target = word_dict[seq[-1]] # 'e' is target\n",
    "        input_batch.append(np.eye(n_class)[input]) #独热\n",
    "        target_batch.append(target)\n",
    "    print('input_batch被处理为的形状是:',Variable(torch.Tensor(input_batch)).shape) #输出一下形状,这里的顺序是(batch,输入维度,输出维度)\n",
    "    return Variable(torch.Tensor(input_batch)), Variable(torch.LongTensor(target_batch))\n",
    "flag=False\n",
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextLSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden,) \n",
    "        '''input_size – 输入数据的维度\n",
    "hidden_size – 隐藏层的大小（即隐藏层节点数量），输出向量的维度等于隐藏节点数\n",
    "num_layers – 循环层数目,默认为1\n",
    "bias – 默认为True,\n",
    "batch_first – 默认为False，是否把batch放在第一维\n",
    "dropout – 默认为0,如果非0，就在除了输出层插入Dropout。\n",
    "bidirectional – 是否为双向LSTM'''\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
    "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype)) #这两层为MLP\n",
    "    flag1 =False\n",
    "    def forward(self, X):\n",
    "        input = X.transpose(0, 1)  # X : [n_step, batch_size, n_class] 交换维度的数据\n",
    "\n",
    "        hidden_state = Variable(torch.zeros(1, len(X), n_hidden))   # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        cell_state = Variable(torch.zeros(1, len(X), n_hidden))     # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        if self.flag1==False:\n",
    "            print('input.shape:',input.shape,'RNN输入维度,batch,输出维度','hidden_state.shape:',hidden_state.shape,'(RNN层数*深度, batch, 隐藏层维度)')\n",
    "            self.flag1=True\n",
    "        '''输入数据需要按如下形式传入 input, (h_0,c_0)\n",
    "         LSTM的参数:input: 输入数据，即上面例子中的一个句子（或者一个batch的句子），其维度形状为 (seq_len, batch, input_size)\n",
    "seq_len: 句子长度，即单词数量，这个是需要固定的。当然假如你的一个句子中只有2个单词，但是要求输入10个单词，这个时候可以用torch.nn.utils.rnn.pack_padded_sequence()或者torch.nn.utils.rnn.pack_sequence()来对句子进行填充或者截断。\n",
    "batch：就是你一次传入的句子的数量\n",
    "input_size: 每个单词向量的长度，这个必须和你前面定义的网络结构保持一致\n",
    "\n",
    "h_0：维度形状为 (num_layers * num_directions, batch, hidden_size):\n",
    "第一个参数的含义num_layers * num_directions， 即LSTM的层数乘以方向数量。这个方向数量是由前面介绍的bidirectional决定，如果为False,则等于1；反之等于2。\n",
    "batch：同上\n",
    "hidden_size: 隐藏层节点数\n",
    "\n",
    "c_0：维度形状为 (num_layers * num_directions, batch, hidden_size),各参数含义和h_0类似。'''\n",
    "\n",
    "        outputs, (_ , _ ) = self.lstm(input, (hidden_state, cell_state))\n",
    "        \n",
    "        ''''output：维度和输入数据类似，只不过最后的feature部分会有点不同，即 (seq_len, batch, num_directions * hidden_size)\n",
    "这个输出tensor包含了LSTM模型最后一层每个time step的输出特征，比如说LSTM有两层，那么最后输出的是,表示第二层LSTM每个time step对应的输出。\n",
    "另外如果前面你对输入数据使用了torch.nn.utils.rnn.PackedSequence,那么输出也会做同样的操作编程packed sequence。\n",
    "对于unpacked情况，我们可以对输出做如下处理来对方向作分离output.view(seq_len, batch, num_directions, hidden_size), 其中前向和后向分别用0和1表示Similarly, the directions can be separated in the packed case'''\n",
    "        \n",
    "        global flag\n",
    "        if flag==False:\n",
    "            \n",
    "            print('outputs[0]:\\n\\n',outputs[0])\n",
    "            print('batch1[0]:\\n\\n',batch1[0])\n",
    "            print('num_directions1[0]:\\n\\n',num_directions1[0])\n",
    "            flag=True\n",
    "        outputs = outputs[-1]  # [batch_size, n_hidden]\n",
    "        model = torch.mm(outputs, self.W) + self.b  # model : [batch_size, n_class]\n",
    "        return model\n",
    "\n",
    "input_batch, target_batch = make_batch(seq_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "sX4M-kbQZtmS",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "outputId": "ce4ac3be-dc76-4ae3-8db3-5df57944a5a2",
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "input.shape: torch.Size([3, 10, 26]) RNN输入维度,batch,输出维度 hidden_state.shape: torch.Size([1, 10, 8]) (RNN层数*深度, batch, 隐藏层维度)\ntensor([[[ 0.0201,  0.0694, -0.0057,  0.0396, -0.1485, -0.0943,  0.0833,\n          -0.0261],\n         [-0.0715,  0.0271,  0.0697,  0.0165, -0.2064,  0.0282,  0.0626,\n           0.0361],\n         [ 0.0348,  0.0584,  0.0725,  0.0626, -0.0822, -0.0583,  0.0071,\n          -0.0385],\n         [-0.1085,  0.0292,  0.0830,  0.0352, -0.1512, -0.0061, -0.0092,\n           0.0076],\n         [-0.1154,  0.0359,  0.0869,  0.0132, -0.0918,  0.0375, -0.0293,\n          -0.0392],\n         [-0.0847, -0.0263,  0.0923,  0.0736, -0.1362, -0.0391,  0.0915,\n           0.0118],\n         [-0.1154,  0.0359,  0.0869,  0.0132, -0.0918,  0.0375, -0.0293,\n          -0.0392],\n         [-0.0847, -0.0263,  0.0923,  0.0736, -0.1362, -0.0391,  0.0915,\n           0.0118],\n         [-0.0847, -0.0263,  0.0923,  0.0736, -0.1362, -0.0391,  0.0915,\n           0.0118],\n         [-0.0180, -0.0276, -0.0065,  0.1041, -0.0458, -0.0920,  0.0764,\n           0.0340]],\n\n        [[-0.0217,  0.0568,  0.0850,  0.0476, -0.2016, -0.1287,  0.1178,\n          -0.0937],\n         [-0.0213,  0.0643,  0.0911,  0.0579, -0.2070, -0.0596,  0.0438,\n           0.0924],\n         [-0.0514,  0.0808,  0.1314,  0.0551, -0.1260, -0.0058,  0.0564,\n           0.0068],\n         [-0.1068,  0.0752,  0.1394,  0.0278, -0.1416,  0.0224,  0.0369,\n           0.0329],\n         [-0.1159,  0.0770,  0.1326,  0.0236, -0.1161,  0.0445,  0.0314,\n          -0.0019],\n         [-0.1256,  0.0025,  0.1406,  0.0572, -0.1784, -0.1029,  0.1178,\n          -0.0838],\n         [-0.1370, -0.0138,  0.0772,  0.0780, -0.1000, -0.0116, -0.0317,\n           0.0454],\n         [-0.1159,  0.0558,  0.1438,  0.0518, -0.1362,  0.0061,  0.0805,\n           0.0370],\n         [-0.1256,  0.0025,  0.1406,  0.0572, -0.1784, -0.1029,  0.1178,\n          -0.0838],\n         [-0.0159,  0.0754,  0.0712,  0.0795, -0.1385, -0.0575,  0.1041,\n           0.0509]],\n\n        [[ 0.0476,  0.0276,  0.1268,  0.0039, -0.1926, -0.0517,  0.1329,\n          -0.0711],\n         [ 0.0095,  0.0711,  0.1048,  0.0798, -0.2355, -0.0968,  0.0430,\n           0.1249],\n         [-0.0890,  0.0625,  0.1488,  0.0512, -0.1906, -0.0928,  0.1063,\n          -0.0893],\n         [-0.1522,  0.0550,  0.1197, -0.0006, -0.1358, -0.0074,  0.0719,\n           0.0644],\n         [-0.0677,  0.1229,  0.0716,  0.0564, -0.1548, -0.0427,  0.1072,\n          -0.0447],\n         [-0.0816,  0.0998,  0.1256,  0.0634, -0.1887, -0.0485,  0.1225,\n          -0.0311],\n         [-0.0916,  0.0742,  0.0611,  0.0681, -0.1400, -0.0724,  0.0731,\n          -0.0065],\n         [-0.0476,  0.0989,  0.0494,  0.0597, -0.2107, -0.0943,  0.1065,\n          -0.0288],\n         [-0.0807, -0.0239,  0.0545,  0.1271, -0.1256, -0.1320,  0.1224,\n          -0.0188],\n         [-0.0564,  0.0441,  0.1320,  0.0648, -0.1976, -0.1166,  0.1320,\n          -0.0629]]], grad_fn=<StackBackward>)\nEpoch: 0100 cost = 2.587348\nEpoch: 0200 cost = 1.368845\nEpoch: 0300 cost = 0.809449\nEpoch: 0400 cost = 0.501812\nEpoch: 0500 cost = 0.320106\nEpoch: 0600 cost = 0.197544\nEpoch: 0700 cost = 0.128247\nEpoch: 0800 cost = 0.082599\nEpoch: 0900 cost = 0.054571\nEpoch: 1000 cost = 0.039507\n['mak', 'nee', 'coa', 'wor', 'lov', 'hat', 'liv', 'hom', 'has', 'sta'] -> ['e', 'd', 'l', 'd', 'e', 'e', 'e', 'e', 'h', 'r']\n"
    }
   ],
   "source": [
    "\n",
    "model = TextLSTM()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "output = model(input_batch)\n",
    "\n",
    "# Training\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "inputs = [sen[:3] for sen in seq_data]\n",
    "\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "print(inputs, '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TextLSTM-Torch.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}